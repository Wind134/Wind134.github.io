---
title: 进程的设计
author: Ping
math: true
date: 2023-08-19 10:33:00 +0800
categories: [操作系统]
tags: [操作系统]
---

## 设计进程的意义

在刚开始学习计算机的时候，从来就没有真正考虑过进程对于资源调度的意义，出于准备秋招的契机，在这里记录一下个人的理解；

先抛出一个问题，为什么操作系统需要创建进程去处理资源的运行？<font color=red>操作系统直接去管理程序不好吗</font>？我也可以实现对程序的并发处理啊。

我们把整个计算机看成是一家超大公司，那操作系统显然是这家公司的老板，而要执行的程序就是这家公司的某个部门，当然，这个类比相对比较特殊的一点是，老板需要亲自负责每个部门的工作，再有，每个部门的工作都依赖公司唯一的机器。其次，为什么说是部门而不是员工呢？因为后面还有线程的概念，后续会补充这一点，一个部门需要完成各式各样的工作任务，假设并不存在进程，且公司有1000个部门，因为进程不存在，所以老板需要亲自管理这1000个部门，然后我们现在假设这么一种场景：
- 部门1跟部门999现在有很重要的工作任务，需要老板去处理，1~999之间的部门也有一些需要完成的工作，999部门的事情很紧急，等到执行它的时候再去执行，需要等999个部门干完自己的活，这样显然不现实；
- 现在部门1跟部门999都要完成自己的任务，由于部门之间并不存在一个负责人，部门现在为了自己的任务，疯狂抢夺公司那唯一的机器，1~999之间的部门一看这情况，直接开摆，不就是开抢嘛，我们也行，于是也加入进来，这个公司一片混乱，老板直接崩溃；
- 这个时候怎么办，招人呗，他觉得需要管管每个人部门了，毫无秩序怎么行，那谁管呢，老板自己？老板自己管也不是不行，但是1000个部门啊，等老板一个一个管理，那999号部门得等多久才能执行自己的工作，再有，老板在管理某个部门的时候，其他部门岂不是又来随便抢夺资源，也许会说，在老板管理某个部门的时候，其他部门状态冻结，但冻结的状态总得有人保存，老板保存，行，但保存那么多，不太行，效率真的很低，于是思来想去决定给每个部门安排一个部门经理，现在把每个部门都交给部门经理，部门在部门经理的指挥下，安分有序工作，老板就负责管理部门经理，老板从管理1000个部门的大大小小的事务上抽出身来，只管理这1000个部门经理，老板再也不需要时刻监督每个部门的情况了，现在他需要管理的是部门经理，部门经理有什么需求向老板提出，老板为部门经理解决需求，而部门经理，就是进程；
- <font color=red>这，就是进程的意义所在！</font>

## 进程的资源分配

有了进程，操作系统后续所有的资源分配都可以基于进程，接下来同样用这个例子描述资源的分配；

- 老板在有了部门经理之后，可以对每个部门的工作状态放心了，但是现在又遇到了新的问题，是不是一定需要每个部门经理管理的工作全部做完才去处理下一个部门的工作呢？
- 一个一个等下去也不是办法，因此他决定制定一种规则，这个规则使得我不需要等待每个部门工作全部做完再去处理下一个部门，而是将公司机器完成一个任务的时间划分为多个小的时间片，分配给每个部门的时间片到了之后，保存好这个部门目前的进度，保存进度的任务得老板来干，但是因为有了<font color=red>部门经理</font>的存在，老板只需要保存部门经理给定的部门状态信息即可，这么一个步骤，大致上就是<font color=red>操作系统对进程上下文切换</font>的过程；
- 假设机器执行一个任务<font color=red>需要占用机器</font>的时间是`1s`，老板拆分这个时间到1000个时间片，这样给每个部门分配的时间是`1ms`，那么处理完1000个也就是`1s`，老板很高兴，`1s`的时间让1000个部门都干活了，宏观上老板在`1s`内实现了1000个部门的并行工作，这种策略，就是CPU资源的<font color=red>时间片轮转</font>的调度策略；

以上这些就是个人对操作系统进程管理的一个比较式理解，整个流程梳理下来，理解清晰了很多，接下来要进行标准的教科书描述了；

## 介绍进程
### 进程的教科书定义

以下的内容基本都是基于汤子瀛等人编撰的《计算机操作系统》-第四版；

操作系统配置了一个称为**进程控制块(Process Control Block, PCB)**的数据结构使得参与并发执行的每个程序能够独立运行，系统正是利用PCB来描述进程的基本情况和活动过程，在系统调用中，`fork()`所创建出的`PID`也属于PCB的一部分，它是一个标识符，操作系统用它来唯一识别一个进程。通常来说，由
- 程序代码段
- 数据段
- 进程控制块

三部分构成了进程实体，这个进程实体一般就称之为进程，当然严格来说进程的定义是多种多样的，但是我们需要把握的一个核心点就是，进程是计算机进行资源分配的基本单位，但是并非CPU调度的基本单位，由于线程的存在，线程可以共享进程的资源，但是线程本身也是一个独立的执行流；

### 进程的基本状态及转换

进程的生命周期一般而言包括了三种状态：
- 就绪态：进程一旦获得了CPU就能进入工作状态；
- 执行态：进程获得CPU后正在执行的状态；
- 阻塞态：执行态的进程由于发生某事件(如I/O请求)等暂时无法继续执行的状态；

阻塞态的进程没有必要一直占据着CPU，这个时候就会将其阻塞，并放入到一个队列，阻塞的进程多了就会形成阻塞队列；

阻塞态的进程在发生的事件完成之后，就从阻塞队列转移到就绪队列；

以上基本就大致描述了三个状态的转换过程；

当然教科书上的描述就更加细节了，在三种基本状态的基础上还会加入一个挂起操作，挂起操作可以干涉上面任意的基本状态；

## 进程控制

在操作系统中，有一个被称为**原语**的概念，所谓原语就是一个不能被打断的操作，依靠这个操作，实现对进程的控制；

在描述对进程的控制之前，先讲讲操作系统中的两种状态：
- 系统态：又称为管态，具有特权，能执行一切指令，操作系统本身的权限级别比较高，因此操作系统相关的程序一定都在系统态中执行；
- 用户态：又称目态，它仅仅能执行规定的指令，如果它想要去控制硬件，比如输出一段文字，那就得通过系统调用，把这个要求传递给操作系统，由系统负责处理这个调度；

后面会针对这部分进行更加详细的描述；

说回进程控制，首先就是进程创建了，进程创建就是一个很显然的原语过程，不能被中断，进程创建分为以下几个过程：
- 分配进程控制块PCB；
- 为新进程分配其运行所需资源，包括内存，文件，IO设备和CPU时间等；
    - 如果是`fork()`操作，那么资源则来源于其父进程，内存也会重新分配，但是内容基本与父进程保持一致；
- 初始化进程控制块PCB，这部分的工作主要是初始化处理器状态信息等，比如将程序入口地址保存进PC计数器；
- 最终正常来说，进程会被插入就绪队列；

进程创建成功之后，理论上接下来要去CPU执行了，但这个时候会遇到的一个问题是，比如我一个*hello world程序*，操作系统知道了`main`函数的逻辑入口地址，但是程序本身是装载在磁盘的，要执行这段程序，需要将程序装入内存，操作系统管理内存的基本单位是页，装入这段程序的过程中就会发生缺页中断，需要将程序载入内存，在等待载入的过程中，该进程会被转为阻塞态，直到中断处理完成，也就是说，进程刚刚进入执行态就被阻塞了，缺页中断完成后，再由中断处理程序将该进程唤醒；

在上述的过程中，阻塞和唤醒的过程都是原语操作，这个操作极其重要，不能被打断；

进程的创建以及终止过程同样是这个性质；

这部分在之前学习的时候全是理论，其中应该结合一些实践过程去理解的；

## 进程的地址空间

千言万语不如一张图标的展示更加直观：

请了一张好图，下面是操作系统整个进程的虚拟地址空间的一个大致展示：

<a name="diagram-1"></a>
![](/assets/img/OS/进程的地址空间.svg)

操作系统为每个运行的进程分配了独立的虚拟内存空间，操作系统将整个内存划分为两个区域：
- **内核空间:** 属于权限极高的进程空间，这部分不允许用户层的任何软件染指，用户进程需要用到内核空间的功能，可以，但是需要经过系统层面的严格把关，如系统调用；
- **用户空间:** 这部分就是用户进程自由挥洒青春的空间，但是也需要遵循一些规范，比如防内存泄漏，每个进程空间都需要遵循调度的原则等；

对于用户进程空间的分配，我们基于该图，基本就能看到，⼀个程序由命令行参数和环境变量、栈、共享区区、堆、BSS段、数据段、代码段组成，接下来逐步介绍各个部分的作用：
- `命令行参数和环境变量`——命令行参数是指从命令行执行程序的时候，给程序的参数。
- `栈区`——存储局部变量、函数参数值，栈有一个特点就是从<font color=red>高地址向低地址</font>增长，同时也是一块连续的空间。
- `共享区`——大多时候留来给文件映射准备，比如`mmap()`函数就是用来将文件内容全部映射到内存的一个系统调用函数；
- `堆区`——动态申请内存用，堆与栈相反，从<font color=red>低地址向高地址</font>增长；
- `BSS段`——存放程序中未初始化的全局变量和静态变量的⼀块内存区域。
- `数据段`——存放程序中已初始化的全局变量和静态变量的⼀块内存区域。
- `代码段`——存放程序执⾏代码的⼀块内存区域，这部分区域<font color=red>只读</font>，代码段的头部还会包含⼀些只读的常数变量。

关于堆和栈为何地址延申的方向相反，后面会专门出一片文章来分析这么设计的缘由，个人认为主要是为了可变参的使用；

## 进程同步

系统中多道程序并发执行，确实可以显著提高系统的吞吐量，但同时也使得系统更加复杂，因此需要一定的同步机制进行妥善管理，有些程序执行的先后顺序必须要确定；

此外，对系统中某些资源的访问必须要有先后，比如典型的生产者——消费者模型，消费者要消费里面的东西，必须有一个前提就是里面有东西；

此外，接下来给出一个例子，这段代码没有加任何的同步机制：
```c
#include <stdio.h>
#include <sys/types.h>
#include <unistd.h>

int main() {
    int num_processes = 0;
    
    // 循环执行三次fork()函数
    for (int i = 0; i < 3; i++) {
        pid_t pid = fork();
        if (pid == 0) {
            // 子进程打印出自己所处的循环次数之后就不再继续fork了
            // 也就是说子进程不参与fork，只有父进程参与，也就是说，子进程进入不到下一个循环了
            // 同时在这里子进程继承的num_processes上次循环后num_processes的值
            printf("子进程 %d\n", i + 1);
            break;
        } else if (pid > 0) {
            // 父进程
            num_processes++;
        } else {
            // fork()函数调用失败
            printf("fork()函数调用失败\n");
            return 1;
        }
    }
    // 父进程打印子进程数量
    if (num_processes > 0) {
        printf("总共创建了%d个子进程\n", num_processes);
    }
    return 0;
}

```
理论上说，这段代码你每次运行，都会得到一个不同的输出结果，而之所以输出不同的结果，根源上就是因为并行的策略；

(对这段代码的分析，后续会出一篇专门的文章)

接下来会介绍的就是操作系统中的同步机制了，在介绍之前，首先提及的是所有同步机制应该遵循的四条原则：
- 空闲让进
- 忙则等待
- 有限等待
- 让权等待

(未完待续)