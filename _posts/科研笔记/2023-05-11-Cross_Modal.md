---
title: Spatio-channel Attention Blocks
author: Ping
math: true
date: 2023-05-11 11:08:00 +0800
categories: [科研笔记, 跨模态]
tags: [科研, 人群计数]
---

## 方法引入

### 已有方法的局限性

尽管在人群计数中使用`RGB`之外的某些模态特征有许多优点，但现今的跨模态人群计数的效果还是有提升的空间。

目前而言的提升主要体现在以下方面：
- 群体计数数据来自不同的模态，这意味着它们在特征分布和以及特征性质上存在显著差异。如何有效地识别它们的差异并统一多模态的信息仍然是跨模态人群计数中的一个可优化问题。此外，由于除`RGB`以外的其他模态不是固定的(热度模态和深度模态)，因此自适应地捕获模态内和模态间的相关性也是可以延申的一个方向；
- 对于跨模态人群计数，除了`RGB`之外，还可提供各种类型的其他模态。此时，从头开始构建一个跨模态模型架构。我们已经看到，通过迁移学习或模型调优，模型架构在各个领域显示出了有希望的结果。在这方面，如果广泛使用的基于`RGB`的单模态体系结构可以扩展到没有附加功能的多模态体系结构，我们就可以减轻模型体系结构的设计选择的工作量，这其实是一个比较深远的工作方向；

### 本文方法
该文章提出了一种新的即插即用模块，即由两个主要模块组成的空间通道注意力`CSCA`模块。

- 首先，空间级跨模态注意力`SCA`模块利用了一种基于`查询`、`键`和`值`三联体的注意机制，广泛应用于非局部模型。
    
    - 对于非局部模型的理解不难，我们广泛使用的卷积神经网络`CNN`只关注那一小块的信息，此为局部信息；
    - 而现如今广泛发展的注意力机制则允许模型在处理序列数据时分配不同权重给不同位置的输入，从而更好地捕捉全局依赖关系；

    此外，一些传统的<font color=red>非局部模型</font>被设计用来提取单一模态中的自注意力，它们并不一定适合跨模态任务，可能受限于计算复杂度或者其他的一些因素；为了解决这些问题，本文的`SCA`模块被有意地设计来理解跨模态特征之间的相关性，并通过所提出的特征重新组装方案来减少计算开销。
    
    - 但在我看来，这样真的实现了对计算复杂度的降低吗？这是一个可以探讨的问题；
    - 下面针对这情况有说明，可以浅显的理解到如何实现复杂度的降低；

    给定一个大小为$C×H×W$的输入特征图，显然可以很显然的计算到，一个典型的非局部块的计算复杂度为$O(CH^2W^2$)，而`SCA`块可以将复杂度降低到($CH^2W^2/G_l$)。这里的$G_l$指的是主干网络第$l$个块的空间重组系数`Spatial Re-assembling Factor`。

    这么处理是仅仅为了降低计算复杂度吗？还是说会有一些其他的用处；

重要的是，在各种视觉任务中广泛使用的<font color=red>非局部块</font>侧重于特征的空间相关性，但不直接在通道层面执行特征校准。

- 为了给这一类仅考虑空间相关性的特征的非局部块补充通道级别的特征信息，提出了一个空间级特征聚合模块`CFA`，该模块在空间通道层面自适应地调整相关的特征。

以往的研究使用金字塔池化特征差异来测量多模态特征变化或微调密度图，与此不同，`SCA`模块被设计在空间上捕获多模态之间的全局特征相关性，以缓解多模态数据中的错位。

该方法还有一个创新之处在于并没有为其他模态信息添加辅助任务，而是通过`CFA`模块沿着通道维度动态聚合互补特征。

## 模块结构

首先从全局层面看该模型的处理框架：

![看不到啦](https://pics-for-blog.oss-cn-hangzhou.aliyuncs.com/Research/CSCA.png)
_CSCA_

我们基本可以按照这么个思路来了解整个流程：

- 整个框架有五大块，前面四个块是特征学习的核心，`RGB`与`T`模态起初送入第一个块进行处理，通过`CSCA`模块，`RGB`与`T`模态可以相互了解到对方的特征信息，之后返回相应的信息；
- 这种处理过程要经历四次，之后，两个模态应该可以被认为是充分学习到对面的有关特征，从而实现充分的互补，最终可以得到一个特征融合之后的图，将该图作为输入，经由`Block5`处理之后，生成密度图，最终得到详细的计数结果；

接下来针对这两个模块(`SCA`与`CFA`)进行详细的拆解：

### 空间层面的跨模态注意力

首先看`SCA`模块的架构图：

![没有图片](https://pics-for-blog.oss-cn-hangzhou.aliyuncs.com/Research/SCA%E7%9A%84%E7%BB%86%E8%8A%82.png)

如果对注意力机制有深入的了解，可以很明显的看出这是对传统非局部块的拓展，也就是说传统的非局部块只针对单一模态的特征进行注意力的提取，非局部块的引入有助于模型更好地理解全局上下文，特别是在处理长距离依赖性时。这对于某些任务，如图像分割、视频理解和语义分析，尤其重要。

### 通道层面的特征聚合



